{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HR3Z3_qQhwC",
        "outputId": "9b8a8d4e-a50e-40f0-bc03-2a62778a7a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "train_small = load_dataset(\n",
        "    \"parquet\",\n",
        "    data_files=\"/content/drive/MyDrive/Patent_Data/bigpatent_train_small.parquet\"\n",
        ")[\"train\"]\n",
        "train_small = train_small.select(range(5000))\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "MAX_INPUT  = 256\n",
        "MAX_TARGET = 64\n",
        "\n",
        "def preprocess(batch):\n",
        "    inputs  = tokenizer(batch[\"description\"], max_length=MAX_INPUT, truncation=True)\n",
        "    targets = tokenizer(batch[\"abstract\"],    max_length=MAX_TARGET, truncation=True)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_train = train_small.map(preprocess, batched=True, remove_columns=train_small.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "import time\n",
        "\n",
        "# 1) Take a smaller subset for now\n",
        "tokenized_small = tokenized_train.select(range(800))   # try 500â€“800\n",
        "\n",
        "# 2) Choose device: if GPU keeps OOM-ing, use \"cpu\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS     = 1\n",
        "LR         = 5e-5\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized_small,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "last_print = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # hard cap: only train on first 300 batches max\n",
        "        if batch_idx >= 300:\n",
        "            break\n",
        "\n",
        "        # move to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # free some GPU memory if applicable\n",
        "        if device == \"cuda\":\n",
        "            del batch, outputs, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # heartbeat every ~20 seconds\n",
        "        if time.time() - last_print > 20:\n",
        "            print(f\"[Epoch {epoch+1}] Step {step}, Batch {batch_idx+1}, Loss {total_loss/step:.4f}\")\n",
        "            last_print = time.time()\n",
        "\n",
        "    avg_loss = total_loss / max(1, step)\n",
        "    print(f\"Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhJlsXL7QjXn",
        "outputId": "55beb2f9-ae38-4ff4-cce4-b073f36312d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Step 1, Batch 1, Loss 0.8822\n",
            "[Epoch 1] Step 3, Batch 3, Loss 2.7765\n",
            "[Epoch 1] Step 5, Batch 5, Loss 3.4779\n",
            "[Epoch 1] Step 7, Batch 7, Loss 3.6577\n",
            "[Epoch 1] Step 10, Batch 10, Loss 3.6155\n",
            "[Epoch 1] Step 12, Batch 12, Loss 3.8042\n",
            "[Epoch 1] Step 14, Batch 14, Loss 3.8242\n",
            "[Epoch 1] Step 16, Batch 16, Loss 3.9589\n",
            "[Epoch 1] Step 18, Batch 18, Loss 3.9964\n",
            "[Epoch 1] Step 20, Batch 20, Loss 4.1191\n",
            "[Epoch 1] Step 22, Batch 22, Loss 4.1347\n",
            "[Epoch 1] Step 24, Batch 24, Loss 4.0970\n",
            "[Epoch 1] Step 26, Batch 26, Loss 4.1112\n",
            "[Epoch 1] Step 28, Batch 28, Loss 4.1644\n",
            "[Epoch 1] Step 30, Batch 30, Loss 4.2161\n",
            "[Epoch 1] Step 32, Batch 32, Loss 4.2817\n",
            "[Epoch 1] Step 34, Batch 34, Loss 4.3913\n",
            "[Epoch 1] Step 36, Batch 36, Loss 4.4689\n",
            "[Epoch 1] Step 38, Batch 38, Loss 4.5239\n",
            "[Epoch 1] Step 40, Batch 40, Loss 4.5675\n",
            "[Epoch 1] Step 42, Batch 42, Loss 4.6095\n",
            "[Epoch 1] Step 44, Batch 44, Loss 4.6638\n",
            "[Epoch 1] Step 46, Batch 46, Loss 4.6866\n",
            "[Epoch 1] Step 48, Batch 48, Loss 4.7082\n",
            "[Epoch 1] Step 50, Batch 50, Loss 4.7135\n",
            "[Epoch 1] Step 52, Batch 52, Loss 4.7280\n",
            "[Epoch 1] Step 55, Batch 55, Loss 4.7082\n",
            "[Epoch 1] Step 57, Batch 57, Loss 4.7160\n",
            "[Epoch 1] Step 59, Batch 59, Loss 4.7122\n",
            "[Epoch 1] Step 61, Batch 61, Loss 4.7155\n",
            "[Epoch 1] Step 63, Batch 63, Loss 4.7112\n",
            "[Epoch 1] Step 65, Batch 65, Loss 4.7313\n",
            "[Epoch 1] Step 67, Batch 67, Loss 4.7382\n",
            "[Epoch 1] Step 69, Batch 69, Loss 4.7664\n",
            "[Epoch 1] Step 72, Batch 72, Loss 4.7897\n",
            "[Epoch 1] Step 74, Batch 74, Loss 4.7661\n",
            "[Epoch 1] Step 76, Batch 76, Loss 4.7511\n",
            "[Epoch 1] Step 78, Batch 78, Loss 4.7722\n",
            "[Epoch 1] Step 80, Batch 80, Loss 4.7868\n",
            "[Epoch 1] Step 82, Batch 82, Loss 4.7908\n",
            "[Epoch 1] Step 84, Batch 84, Loss 4.8004\n",
            "[Epoch 1] Step 86, Batch 86, Loss 4.7986\n",
            "[Epoch 1] Step 88, Batch 88, Loss 4.7958\n",
            "[Epoch 1] Step 90, Batch 90, Loss 4.7991\n",
            "[Epoch 1] Step 92, Batch 92, Loss 4.8119\n",
            "[Epoch 1] Step 94, Batch 94, Loss 4.8120\n",
            "[Epoch 1] Step 96, Batch 96, Loss 4.8048\n",
            "[Epoch 1] Step 98, Batch 98, Loss 4.8090\n",
            "[Epoch 1] Step 101, Batch 101, Loss 4.8278\n",
            "[Epoch 1] Step 103, Batch 103, Loss 4.8325\n",
            "[Epoch 1] Step 105, Batch 105, Loss 4.8316\n",
            "[Epoch 1] Step 108, Batch 108, Loss 4.8191\n",
            "[Epoch 1] Step 110, Batch 110, Loss 4.8177\n",
            "[Epoch 1] Step 112, Batch 112, Loss 4.8109\n",
            "[Epoch 1] Step 114, Batch 114, Loss 4.8154\n",
            "[Epoch 1] Step 116, Batch 116, Loss 4.8197\n",
            "[Epoch 1] Step 118, Batch 118, Loss 4.8221\n",
            "[Epoch 1] Step 120, Batch 120, Loss 4.8112\n",
            "[Epoch 1] Step 122, Batch 122, Loss 4.8077\n",
            "[Epoch 1] Step 124, Batch 124, Loss 4.8068\n",
            "[Epoch 1] Step 126, Batch 126, Loss 4.8108\n",
            "[Epoch 1] Step 128, Batch 128, Loss 4.8126\n",
            "[Epoch 1] Step 131, Batch 131, Loss 4.8089\n",
            "[Epoch 1] Step 133, Batch 133, Loss 4.8064\n",
            "[Epoch 1] Step 135, Batch 135, Loss 4.7983\n",
            "[Epoch 1] Step 137, Batch 137, Loss 4.7999\n",
            "[Epoch 1] Step 139, Batch 139, Loss 4.8021\n",
            "[Epoch 1] Step 142, Batch 142, Loss 4.7911\n",
            "[Epoch 1] Step 144, Batch 144, Loss 4.7863\n",
            "[Epoch 1] Step 146, Batch 146, Loss 4.7807\n",
            "[Epoch 1] Step 148, Batch 148, Loss 4.7781\n",
            "[Epoch 1] Step 150, Batch 150, Loss 4.7749\n",
            "[Epoch 1] Step 152, Batch 152, Loss 4.7729\n",
            "[Epoch 1] Step 154, Batch 154, Loss 4.7682\n",
            "[Epoch 1] Step 156, Batch 156, Loss 4.7666\n",
            "[Epoch 1] Step 158, Batch 158, Loss 4.7678\n",
            "[Epoch 1] Step 160, Batch 160, Loss 4.7683\n",
            "[Epoch 1] Step 162, Batch 162, Loss 4.7684\n",
            "[Epoch 1] Step 164, Batch 164, Loss 4.7715\n",
            "[Epoch 1] Step 166, Batch 166, Loss 4.7708\n",
            "[Epoch 1] Step 168, Batch 168, Loss 4.7752\n",
            "[Epoch 1] Step 170, Batch 170, Loss 4.7698\n",
            "[Epoch 1] Step 172, Batch 172, Loss 4.7696\n",
            "[Epoch 1] Step 174, Batch 174, Loss 4.7703\n",
            "[Epoch 1] Step 176, Batch 176, Loss 4.7728\n",
            "[Epoch 1] Step 179, Batch 179, Loss 4.7757\n",
            "[Epoch 1] Step 181, Batch 181, Loss 4.7784\n",
            "[Epoch 1] Step 183, Batch 183, Loss 4.7744\n",
            "[Epoch 1] Step 185, Batch 185, Loss 4.7703\n",
            "[Epoch 1] Step 187, Batch 187, Loss 4.7852\n",
            "[Epoch 1] Step 189, Batch 189, Loss 4.7890\n",
            "[Epoch 1] Step 191, Batch 191, Loss 4.7876\n",
            "[Epoch 1] Step 193, Batch 193, Loss 4.7919\n",
            "[Epoch 1] Step 195, Batch 195, Loss 4.7913\n",
            "[Epoch 1] Step 197, Batch 197, Loss 4.7905\n",
            "[Epoch 1] Step 199, Batch 199, Loss 4.7864\n",
            "[Epoch 1] Step 201, Batch 201, Loss 4.7841\n",
            "[Epoch 1] Step 203, Batch 203, Loss 4.7863\n",
            "[Epoch 1] Step 205, Batch 205, Loss 4.7827\n",
            "[Epoch 1] Step 207, Batch 207, Loss 4.7838\n",
            "[Epoch 1] Step 209, Batch 209, Loss 4.7755\n",
            "[Epoch 1] Step 211, Batch 211, Loss 4.7791\n",
            "[Epoch 1] Step 213, Batch 213, Loss 4.7750\n",
            "[Epoch 1] Step 215, Batch 215, Loss 4.7704\n",
            "[Epoch 1] Step 217, Batch 217, Loss 4.7729\n",
            "[Epoch 1] Step 219, Batch 219, Loss 4.7705\n",
            "[Epoch 1] Step 221, Batch 221, Loss 4.7689\n",
            "[Epoch 1] Step 223, Batch 223, Loss 4.7723\n",
            "[Epoch 1] Step 225, Batch 225, Loss 4.7746\n",
            "[Epoch 1] Step 227, Batch 227, Loss 4.7750\n",
            "[Epoch 1] Step 229, Batch 229, Loss 4.7710\n",
            "[Epoch 1] Step 231, Batch 231, Loss 4.7732\n",
            "[Epoch 1] Step 233, Batch 233, Loss 4.7736\n",
            "[Epoch 1] Step 236, Batch 236, Loss 4.7656\n",
            "[Epoch 1] Step 238, Batch 238, Loss 4.7680\n",
            "[Epoch 1] Step 240, Batch 240, Loss 4.7668\n",
            "[Epoch 1] Step 242, Batch 242, Loss 4.7602\n",
            "[Epoch 1] Step 244, Batch 244, Loss 4.7536\n",
            "[Epoch 1] Step 246, Batch 246, Loss 4.7560\n",
            "[Epoch 1] Step 248, Batch 248, Loss 4.7554\n",
            "[Epoch 1] Step 250, Batch 250, Loss 4.7586\n",
            "[Epoch 1] Step 252, Batch 252, Loss 4.7558\n",
            "[Epoch 1] Step 254, Batch 254, Loss 4.7519\n",
            "[Epoch 1] Step 256, Batch 256, Loss 4.7486\n",
            "[Epoch 1] Step 258, Batch 258, Loss 4.7438\n",
            "[Epoch 1] Step 260, Batch 260, Loss 4.7416\n",
            "[Epoch 1] Step 262, Batch 262, Loss 4.7410\n",
            "[Epoch 1] Step 264, Batch 264, Loss 4.7414\n",
            "[Epoch 1] Step 266, Batch 266, Loss 4.7370\n",
            "[Epoch 1] Step 268, Batch 268, Loss 4.7378\n",
            "[Epoch 1] Step 270, Batch 270, Loss 4.7372\n",
            "[Epoch 1] Step 272, Batch 272, Loss 4.7355\n",
            "[Epoch 1] Step 274, Batch 274, Loss 4.7346\n",
            "[Epoch 1] Step 276, Batch 276, Loss 4.7309\n",
            "[Epoch 1] Step 278, Batch 278, Loss 4.7321\n",
            "[Epoch 1] Step 280, Batch 280, Loss 4.7322\n",
            "[Epoch 1] Step 282, Batch 282, Loss 4.7317\n",
            "[Epoch 1] Step 284, Batch 284, Loss 4.7299\n",
            "[Epoch 1] Step 286, Batch 286, Loss 4.7298\n",
            "[Epoch 1] Step 288, Batch 288, Loss 4.7256\n",
            "[Epoch 1] Step 290, Batch 290, Loss 4.7255\n",
            "[Epoch 1] Step 292, Batch 292, Loss 4.7263\n",
            "[Epoch 1] Step 294, Batch 294, Loss 4.7254\n",
            "[Epoch 1] Step 296, Batch 296, Loss 4.7250\n",
            "[Epoch 1] Step 298, Batch 298, Loss 4.7251\n",
            "Epoch 1 finished. Avg loss: 4.7237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Patent_Data/distilbart_bigpatent_final\"\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\"Model saved to:\", save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9hvyLNoQlxc",
        "outputId": "879bb2b2-6abe-4528-d151-37608ba600d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/Patent_Data/distilbart_bigpatent_final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "for i in range(3):\n",
        "    ex = train_small[i]\n",
        "    inputs = tokenizer(\n",
        "        ex[\"description\"],\n",
        "        max_length=MAX_INPUT,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=MAX_TARGET,\n",
        "            num_beams=4,\n",
        "        )\n",
        "\n",
        "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"=== EXAMPLE {i} ===\")\n",
        "    print(\"GOLD:\", ex[\"abstract\"][:400])\n",
        "    print(\"PRED:\", pred)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDeMXmxXQqyb",
        "outputId": "bf2af8e3-bd6a-43ca-f905-0b7faa471b30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== EXAMPLE 0 ===\n",
            "GOLD: An apparatus for recording and reproducing video and audio signals in either analog or digital form. The heads are arranged to conform with existing analog standards, such as VHS, and yet are also able to record in digital form in adjacent non-overlapping tracks. In a preferred embodiment, one pair of heads is arranged with a small azimuth angle (+/-6 degrees), and another pair is arranged with a \n",
            "PRED: The present invention provides a method and apparatus for the use of a computer system. The computer system includes a system, a system and a network network network, a network, and an network network. The network network includes a network system, the network network system and the network system. A network network and network\n",
            "\n",
            "=== EXAMPLE 1 ===\n",
            "GOLD: A system, method and program product for interactively scheduling and negotiating meetings wherein an active agent program accepts meeting criteria from a meeting requester and interacts with invitees to resolve availability according to the meeting criteria. The agent transmits the negotiated meeting schedule to invitees and optionally requires confirmation from invitees.\n",
            "PRED: The present invention provides a method and apparatus for the use of a computer system. The computer system includes a system, a system and a network network network, a network, and an network network. The network network includes a network system, the network network system and the network system. A network network and network\n",
            "\n",
            "=== EXAMPLE 2 ===\n",
            "GOLD: Highly doped N- and P-type wells (16a, 16b) in a first silicon layer (16) on an insulator layer (14) of a SIMOX substrate (10). Complementary MOSFET devices (52,54,58,62) are formed in lightly doped N- and P-type active areas (22a, 22b) in a second silicon layer (22) formed on the first silicon layer (16). Adjacent active areas (22a, 22b) and underlying wells (16a, 16b) are isolated from each othe\n",
            "PRED: The present invention provides a method and apparatus for the use of a computer system. The computer system includes a system, a system and a network network network, a network, and an network network. The network network includes a network system, the network network system and the network system. A network network and network\n",
            "\n"
          ]
        }
      ]
    }
  ]
}